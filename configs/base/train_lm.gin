DEVICE=[1]
TRAIN_BATCH_SIZE=48
TRAIN_DATALOADER_NUM_WORKERS=0
VAL_BATCH_SIZE=16
VAL_DATALOADER_NUM_WORKERS=0
INITIAL_CHECKPOINT='last'
TRAINING_STEPS=500000
GRAD_ACC=1
CHECKPOINT_INTERVAL=10000
PRECISION=16

$keys_not_saved=['datasets','dataloaders']
execute_pipeline:
    tasks = [@encodecmae_tasks.utils.set_seed,
             @encodecmae_tasks.data.load_dataset,
             @encodecgpt_tasks.segment_dataset,
             @encodecgpt_tasks.make_prompt,
             @encodecmae_tasks.data.get_dataloaders,
             @encodecmae_tasks.fit_model]
    execution_order = 'sequential'

get_dataloaders:
    split_function=@encodecgpt_tasks.use_partition
    dataset_cls={'train': @train/encodecmae_tasks.data.DictDataset, 'validation': @val/encodecmae_tasks.data.DictDataset}
    dataloader_cls={'train': @train/torch.utils.data.DataLoader, 'validation': @val/torch.utils.data.DataLoader}

train/torch.utils.data.DataLoader:
    shuffle=True
    batch_size=%TRAIN_BATCH_SIZE
    num_workers=%TRAIN_DATALOADER_NUM_WORKERS

val/torch.utils.data.DataLoader:
    shuffle=False
    batch_size=%VAL_BATCH_SIZE
    num_workers=%VAL_DATALOADER_NUM_WORKERS

encodecmae_tasks.fit_model:
    trainer_cls=@pl.Trainer
    checkpoint_folder='checkpoints'
    from_checkpoint=%INITIAL_CHECKPOINT

pl.Trainer:
    logger=@pl.loggers.CSVLogger()
    devices=%DEVICE
    callbacks=[@pl.callbacks.ModelCheckpoint(), @pl.callbacks.LearningRateMonitor()]
    max_steps=%TRAINING_STEPS
    accelerator='gpu'
    accumulate_grad_batches=%GRAD_ACC
    num_sanity_val_steps=1
    val_check_interval=%CHECKPOINT_INTERVAL
    precision=%PRECISION
    check_val_every_n_epoch=None

pl.callbacks.ModelCheckpoint:
    dirpath=%OUTPUT_DIR
    every_n_train_steps=%CHECKPOINT_INTERVAL
    save_top_k=-1 #Keep all the checkpoints

pl.loggers.CSVLogger:
    save_dir=%OUTPUT_DIR
    name='logs'

